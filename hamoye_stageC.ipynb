{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgZ8D8s7SNvT",
        "outputId": "0e5ee6af-b1de-4403-b22e-8b2a54f4c885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "2xNOdQPhSlMF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path in Colab where you uploaded the dataset\n",
        "file_path = '/content/WA_Fn-UseC_-Telco-Customer-Churn.csv'  # Adjust the path as needed\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "WiZWICwrTxa3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'TotalCharges' to numeric and fill missing values with 0\n",
        "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
        "data['TotalCharges'].fillna(0, inplace=True)\n",
        "\n",
        "# Convert 'Churn' to binary values\n",
        "data['Churn'] = (data['Churn'] == 'Yes').astype(int)\n"
      ],
      "metadata": {
        "id": "Ozm2qn5QT31E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the categorical and numerical features\n",
        "categorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
        "               'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "               'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
        "numerical = ['tenure', 'MonthlyCharges', 'TotalCharges']\n"
      ],
      "metadata": {
        "id": "wF5Ruc25VKGZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X = data[categorical + numerical]\n",
        "y = data['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n"
      ],
      "metadata": {
        "id": "98bybL2XU0ip"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numerical features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical] = scaler.fit_transform(X_train[numerical])\n",
        "X_test[numerical] = scaler.transform(X_test[numerical])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "X_train_encoded = encoder.fit_transform(X_train[categorical])\n",
        "X_test_encoded = encoder.transform(X_test[categorical])\n",
        "\n",
        "# Get feature names using get_feature_names_out\n",
        "feature_names = encoder.get_feature_names_out(input_features=categorical)\n",
        "X_train_encoded = pd.DataFrame(X_train_encoded, columns=feature_names)\n",
        "X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QICZzvPUU4XL",
        "outputId": "440687a5-337a-43b1-f424-804c832db63f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.concat([X_train_encoded, X_train[numerical]], axis=1)\n",
        "X_test = pd.concat([X_test_encoded, X_test[numerical]], axis=1)\n"
      ],
      "metadata": {
        "id": "vrYvNNuuVQ12"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# Create and train the HistGradientBoostingClassifier\n",
        "hgb_model = HistGradientBoostingClassifier(random_state=1)\n",
        "hgb_model.fit(X_train_encoded, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "9DY9HpX-WJ7s",
        "outputId": "2d8ee622-2725-4edf-8d54-253deff33846"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HistGradientBoostingClassifier(random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the models\n",
        "models = {\n",
        "    'Extra Trees': ExtraTreesClassifier(random_state=1),\n",
        "    'XGBoost': XGBClassifier(random_state=1),\n",
        "    'LightGBM': LGBMClassifier(random_state=1),\n",
        "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=1)  # Add HistGradientBoostingClassifier\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_encoded, y_train)  # Use X_train_encoded\n",
        "    y_pred = model.predict(X_test_encoded)  # Use X_test_encoded\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "\n",
        "# Print the results\n",
        "for name, accuracy in results.items():\n",
        "    print(f'{name} Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcCfKzZVp_v",
        "outputId": "3ccbcdde-5fca-4627-e62c-d92e05480caa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1521, number of negative: 4113\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269968 -> initscore=-0.994785\n",
            "[LightGBM] [Info] Start training from score -0.994785\n",
            "Extra Trees Accuracy: 0.7566\n",
            "XGBoost Accuracy: 0.7658\n",
            "LightGBM Accuracy: 0.7885\n",
            "HistGradientBoosting Accuracy: 0.7899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# To ignore all warnings:\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Specify the cross-validation method (e.g., StratifiedKFold, LeaveOneOut)\n",
        "cross_val_method = StratifiedKFold(n_splits=5)  # Use StratifiedKFold as an example\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Cross-validation for the model\n",
        "    cv_scores = cross_val_score(model, X_train_encoded, y_train, cv=cross_val_method, scoring='accuracy')\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    cv_precision = cross_val_score(model, X_train_encoded, y_train, cv=cross_val_method, scoring='precision')\n",
        "    cv_recall = cross_val_score(model, X_train_encoded, y_train, cv=cross_val_method, scoring='recall')\n",
        "    cv_f1 = cross_val_score(model, X_train_encoded, y_train, cv=cross_val_method, scoring='f1')\n",
        "\n",
        "    # Print the results\n",
        "    print(f'{name} Cross-Validation Accuracy: {cv_scores.mean():.4f}')\n",
        "    print(f'{name} Cross-Validation Precision: {cv_precision.mean():.4f}')\n",
        "    print(f'{name} Cross-Validation Recall: {cv_recall.mean():.4f}')\n",
        "    print(f'{name} Cross-Validation F1 Score: {cv_f1.mean():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn-F5uu-VvZY",
        "outputId": "d0e714a9-ce47-442c-87a2-97e442cada50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra Trees Cross-Validation Accuracy: 0.7503\n",
            "Extra Trees Cross-Validation Precision: 0.5474\n",
            "Extra Trees Cross-Validation Recall: 0.4326\n",
            "Extra Trees Cross-Validation F1 Score: 0.4832\n",
            "XGBoost Cross-Validation Accuracy: 0.7519\n",
            "XGBoost Cross-Validation Precision: 0.5444\n",
            "XGBoost Cross-Validation Recall: 0.4997\n",
            "XGBoost Cross-Validation F1 Score: 0.5209\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1216, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269803 -> initscore=-0.995625\n",
            "[LightGBM] [Info] Start training from score -0.995625\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269965 -> initscore=-0.994803\n",
            "[LightGBM] [Info] Start training from score -0.994803\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1216, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269803 -> initscore=-0.995625\n",
            "[LightGBM] [Info] Start training from score -0.995625\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269965 -> initscore=-0.994803\n",
            "[LightGBM] [Info] Start training from score -0.994803\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001684 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1216, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269803 -> initscore=-0.995625\n",
            "[LightGBM] [Info] Start training from score -0.995625\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269965 -> initscore=-0.994803\n",
            "[LightGBM] [Info] Start training from score -0.994803\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270024 -> initscore=-0.994499\n",
            "[LightGBM] [Info] Start training from score -0.994499\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1216, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269803 -> initscore=-0.995625\n",
            "[LightGBM] [Info] Start training from score -0.995625\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1217, number of negative: 3291\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269965 -> initscore=-0.994803\n",
            "[LightGBM] [Info] Start training from score -0.994803\n",
            "LightGBM Cross-Validation Accuracy: 0.7726\n",
            "LightGBM Cross-Validation Precision: 0.5929\n",
            "LightGBM Cross-Validation Recall: 0.5029\n",
            "LightGBM Cross-Validation F1 Score: 0.5441\n",
            "HistGradientBoosting Cross-Validation Accuracy: 0.7709\n",
            "HistGradientBoosting Cross-Validation Precision: 0.5891\n",
            "HistGradientBoosting Cross-Validation Recall: 0.4990\n",
            "HistGradientBoosting Cross-Validation F1 Score: 0.5402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Choose a model (e.g., HistGradientBoosting)\n",
        "model = models['HistGradientBoosting']\n",
        "\n",
        "# Train the model on the full training data\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_encoded)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYHHqrUkahtX",
        "outputId": "c8cf281b-1958-41d2-f9b5-2cb458d07ac6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[922 139]\n",
            " [157 191]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# To ignore all warnings:\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Specify the models you want to evaluate\n",
        "models = {\n",
        "    'Extra Trees': ExtraTreesClassifier(random_state=1),\n",
        "    'XGBoost': XGBClassifier(random_state=1),\n",
        "    'LightGBM': LGBMClassifier(random_state=1),\n",
        "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=1)\n",
        "}\n",
        "\n",
        "# Specify the cross-validation methods\n",
        "cv_methods = {\n",
        "    'K-Fold': KFold(n_splits=5, shuffle=True, random_state=1),  # K-Fold with 5 splits\n",
        "    'Stratified K-Fold': StratifiedKFold(n_splits=5, shuffle=True, random_state=1),  # Stratified K-Fold\n",
        "    'Leave-One-Out': LeaveOneOut()  # Leave-One-Out\n",
        "}\n",
        "\n",
        "# Loop over models and cross-validation methods\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "\n",
        "    for cv_method_name, cv_method in cv_methods.items():\n",
        "        print(f\"Cross-Validation Method: {cv_method_name}\")\n",
        "\n",
        "        # Lists to store evaluation metrics across folds\n",
        "        accuracy_scores = []\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        f1_scores = []\n",
        "        confusion_matrices = []\n",
        "\n",
        "        for train_index, test_index in cv_method.split(X_train_encoded, y_train):\n",
        "            X_train_cv, X_test_cv = X_train_encoded.iloc[train_index], X_train_encoded.iloc[test_index]\n",
        "            y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "            # Train the model on the training set of the current fold\n",
        "            model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "            # Make predictions on the test set of the current fold\n",
        "            y_pred_cv = model.predict(X_test_cv)\n",
        "\n",
        "            # Calculate and store evaluation metrics\n",
        "            accuracy_scores.append(accuracy_score(y_test_cv, y_pred_cv))\n",
        "            precision_scores.append(precision_score(y_test_cv, y_pred_cv))\n",
        "            recall_scores.append(recall_score(y_test_cv, y_pred_cv))\n",
        "            f1_scores.append(f1_score(y_test_cv, y_pred_cv))\n",
        "            confusion_matrices.append(confusion_matrix(y_test_cv, y_pred_cv))\n",
        "\n",
        "        # Calculate and print mean and standard deviation of metrics across folds\n",
        "        print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
        "        print(f\"Mean Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
        "        print(f\"Mean Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
        "        print(f\"Mean F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "        # Optionally, you can print or analyze confusion matrices for each fold\n",
        "        for i, cm in enumerate(confusion_matrices):\n",
        "            print(f\"Confusion Matrix (Fold {i + 1}):\\n{cm}\\n\")\n",
        "\n",
        "        print(\"========================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YfEuqpsna8kH",
        "outputId": "430e565c-a6bc-499d-a906-2ee8280669c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Extra Trees\n",
            "Cross-Validation Method: K-Fold\n",
            "Mean Accuracy: 0.7449 ± 0.0063\n",
            "Mean Precision: 0.5341 ± 0.0255\n",
            "Mean Recall: 0.4325 ± 0.0303\n",
            "Mean F1 Score: 0.4774 ± 0.0239\n",
            "Confusion Matrix (Fold 1):\n",
            "[[700 114]\n",
            " [165 148]]\n",
            "\n",
            "Confusion Matrix (Fold 2):\n",
            "[[708 131]\n",
            " [157 131]]\n",
            "\n",
            "Confusion Matrix (Fold 3):\n",
            "[[724 101]\n",
            " [182 120]]\n",
            "\n",
            "Confusion Matrix (Fold 4):\n",
            "[[706 117]\n",
            " [183 121]]\n",
            "\n",
            "Confusion Matrix (Fold 5):\n",
            "[[701 111]\n",
            " [176 138]]\n",
            "\n",
            "========================================\n",
            "Cross-Validation Method: Stratified K-Fold\n",
            "Mean Accuracy: 0.7430 ± 0.0076\n",
            "Mean Precision: 0.5302 ± 0.0185\n",
            "Mean Recall: 0.4162 ± 0.0350\n",
            "Mean F1 Score: 0.4658 ± 0.0272\n",
            "Confusion Matrix (Fold 1):\n",
            "[[705 118]\n",
            " [174 130]]\n",
            "\n",
            "Confusion Matrix (Fold 2):\n",
            "[[708 115]\n",
            " [182 122]]\n",
            "\n",
            "Confusion Matrix (Fold 3):\n",
            "[[702 121]\n",
            " [163 141]]\n",
            "\n",
            "Confusion Matrix (Fold 4):\n",
            "[[720 102]\n",
            " [174 131]]\n",
            "\n",
            "Confusion Matrix (Fold 5):\n",
            "[[718 104]\n",
            " [195 109]]\n",
            "\n",
            "========================================\n",
            "Cross-Validation Method: Leave-One-Out\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-dc06b5f54de5>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Train the model on the training set of the current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Make predictions on the test set of the current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Loop over models and calculate confusion matrices\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_encoded, y_train)  # Train the model on the full training data\n",
        "    y_pred = model.predict(X_test_encoded)  # Make predictions on the test set\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
        "\n",
        "    print(f'Confusion Matrix for {name}:')\n",
        "    print(conf_matrix)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8HhAFHLdlFx",
        "outputId": "892820a7-001a-4c52-e3ba-d02df803f981"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for Extra Trees:\n",
            "[[900 161]\n",
            " [182 166]]\n",
            "\n",
            "Confusion Matrix for XGBoost:\n",
            "[[882 179]\n",
            " [151 197]]\n",
            "\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1521, number of negative: 4113\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269968 -> initscore=-0.994785\n",
            "[LightGBM] [Info] Start training from score -0.994785\n",
            "Confusion Matrix for LightGBM:\n",
            "[[914 147]\n",
            " [151 197]]\n",
            "\n",
            "Confusion Matrix for HistGradientBoosting:\n",
            "[[922 139]\n",
            " [157 191]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Create an Extra Trees Classifier\n",
        "et_classifier = ExtraTreesClassifier(random_state=1)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "n_estimators = [50, 100, 300, 500, 1000]\n",
        "min_samples_split = [2, 3, 5, 7, 9]\n",
        "min_samples_leaf = [1, 2, 4, 6, 8]\n",
        "max_features = ['auto', 'sqrt', 'log2', None]\n",
        "hyperparameter_grid = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'max_features': max_features\n",
        "}\n",
        "\n",
        "# Create the RandomizedSearchCV\n",
        "randomized_search = RandomizedSearchCV(\n",
        "    et_classifier,\n",
        "    param_distributions=hyperparameter_grid,\n",
        "    scoring='accuracy',\n",
        "    n_iter=10,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV to the data\n",
        "randomized_search.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(randomized_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dytJPEz3fWLd",
        "outputId": "c33ebdb7-4c34-46ef-b771-61d627c82e55"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Hyperparameters:\n",
            "{'n_estimators': 1000, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'sqrt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Define the best hyperparameters obtained from RandomizedSearchCV\n",
        "best_hyperparameters = {\n",
        "    'n_estimators': 1000,\n",
        "    'min_samples_leaf': 2,\n",
        "    'min_samples_split': 3,\n",
        "    'max_features': 'auto'\n",
        "}\n",
        "\n",
        "# Create and train the ExtraTreesClassifier model with the best hyperparameters\n",
        "optimal_et_classifier = ExtraTreesClassifier(random_state=1, **best_hyperparameters)\n",
        "optimal_et_classifier.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Calculate the accuracy of the new optimal model\n",
        "optimal_model_accuracy = optimal_et_classifier.score(X_test_encoded, y_test)\n",
        "\n",
        "# Calculate the accuracy of the initial ExtraTreesClassifier model\n",
        "initial_model_accuracy = models['Extra Trees'].score(X_test_encoded, y_test)\n",
        "\n",
        "print(f\"Accuracy of the New Optimal Model: {optimal_model_accuracy:.4f}\")\n",
        "print(f\"Accuracy of the Initial ExtraTreesClassifier Model: {initial_model_accuracy:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if optimal_model_accuracy > initial_model_accuracy:\n",
        "    print(\"The accuracy of the new optimal model is higher than the initial model.\")\n",
        "elif optimal_model_accuracy < initial_model_accuracy:\n",
        "    print(\"The accuracy of the new optimal model is lower than the initial model.\")\n",
        "else:\n",
        "    print(\"The accuracy of the new optimal model is the same as the initial model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDFGTfLwk07u",
        "outputId": "8e8cbf2f-2001-4450-8fee-812719e19640"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the New Optimal Model: 0.7771\n",
            "Accuracy of the Initial ExtraTreesClassifier Model: 0.7566\n",
            "The accuracy of the new optimal model is higher than the initial model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the optimal ExtraTreesClassifier model on the entire training data\n",
        "optimal_et_classifier.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = optimal_et_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame to associate feature names with importances\n",
        "importance_df = pd.DataFrame({'Feature': X_train_encoded.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the two most important features\n",
        "print(\"Two Most Important Features:\")\n",
        "print(importance_df.head(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfAlwUdblhjy",
        "outputId": "8a220b76-85c2-4051-f7aa-5ca09bd72497"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two Most Important Features:\n",
            "                    Feature  Importance\n",
            "34  Contract_Month-to-month    0.124257\n",
            "16        OnlineSecurity_No    0.061179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the LGBM classifier\n",
        "lgbm_classifier = LGBMClassifier(random_state=1)\n",
        "lgbm_classifier.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lgbm_classifier.predict(X_test_encoded)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy on the Test Set using LGBM Classifier: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeWgDq4jmNSh",
        "outputId": "46ab1ef5-e2cf-4734-b678-506b7d7968c8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1521, number of negative: 4113\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001163 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86\n",
            "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 43\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269968 -> initscore=-0.994785\n",
            "[LightGBM] [Info] Start training from score -0.994785\n",
            "Accuracy on the Test Set using LGBM Classifier: 0.7885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNEjQoRbmmeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}